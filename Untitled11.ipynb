{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e182af17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd  #import panda in jupitar notebook\n",
    "papers = pd.read_csv('papers.csv') # read the dataset \n",
    "papers.head() #Number of rows to select."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e186d17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>Maximum Likelihood and the Information\\nBottle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>Half-Lives of EigenFlows for Spectral Clusteri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5762</th>\n",
       "      <td>Optimal Learning for Multi-pass Stochastic Gra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>948</th>\n",
       "      <td>Sparsity of data representation of optimal ker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3402</th>\n",
       "      <td>Brain covariance selection: better individual\\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             paper_text\n",
       "1334  Maximum Likelihood and the Information\\nBottle...\n",
       "1299  Half-Lives of EigenFlows for Spectral Clusteri...\n",
       "5762  Optimal Learning for Multi-pass Stochastic Gra...\n",
       "948   Sparsity of data representation of optimal ker...\n",
       "3402  Brain covariance selection: better individual\\..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove the columns\n",
    "papers = papers.drop(columns=['id', 'title', 'abstract', \n",
    "                              'event_type', 'pdf_name', 'year'], axis=1)  #drop the coulm \n",
    "# sample only 100 papers\n",
    "papers = papers.sample(10)\n",
    "# Print out the first rows of papers\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41aa23b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1334    maximum likelihood and the information\\nbottle...\n",
       "1299    half-lives of eigenflows for spectral clusteri...\n",
       "5762    optimal learning for multi-pass stochastic gra...\n",
       "948     sparsity of data representation of optimal ker...\n",
       "3402    brain covariance selection: better individual\\...\n",
       "Name: paper_text_processed, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import re # Load the regular expression library\n",
    "\n",
    "papers['paper_text_processed'] = papers['paper_text'].map(lambda x: re.sub('[,\\.!?]', '', x))# Remove punctuation\n",
    "papers['paper_text_processed'] = papers['paper_text_processed'].map(lambda x: x.lower())\n",
    "papers['paper_text_processed'].head() # Print out the first rows of papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27f65b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shash\\anaconda3\\lib\\site-packages\\gensim\\similarities\\__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['maximum', 'likelihood', 'and', 'the', 'information', 'bottleneck', 'noam', 'slonim', 'yair', 'weiss']\n"
     ]
    }
   ],
   "source": [
    "import gensim \n",
    "from gensim.utils import simple_preprocess #Convert a document into a list of tokens\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n",
    "data = papers.paper_text_processed.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "print(data_words[:1][0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e3d7818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=10) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=10)\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dfefa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK Stop words\n",
    "# import nltk\n",
    "# nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "def lemmatization(texts, allowed_postags=['NOUN']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4a26115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['likelihood', 'information_bottleneck', 'noam', 'yair', 'weiss', 'school', 'computer', 'science', 'engineering', 'abstract', 'information_bottleneck', 'ib', 'formulation', 'clustering', 'problem', 'joint_distribution', 'method', 'define', 'partition', 'value', 'likelihood', 'mixture', 'model', 'approach', 'clustering', 'problem', 'paper', 'method', 'mapping', 'ml', 'problem', 'mixture_model', 'mapping', 'problem', 'fact', 'input', 'distribution', 'sample', 'size', 'problem', 'case', 'define', 'log', 'likelihood', 'vice_versa', 'value', 'fixed_point', 'transformation', 'result', 'case', 'algorithm', 'problem', 'solution', 'introduction', 'analysis', 'set', 'object', 'partition', 'score', 'function', 'tishby', 'approach', 'problem', 'approach', 'joint_distribution', 'representation', 'information', 'discussion', 'mutual_information', 'variable', 'representation', 'information', 'mutual_information', 'trade', 'quantity', 'representation', 'representation', 'quality', 'cluster', 'fraction', 'information', 'capture', 'mapping', 'lagrange', 'trade', 'compression', 'precision', 'problem', 'solution', 'assumption', 'origin', 'joint_distribution', 'approach', 'mixture', 'modeling', 'mea', 'surement', 'source', 'parameter', 'clustering', 'likelihood', 'estimate', 'parameter', 'probability', 'measurement', 'source', 'probability', 'clustering', 'value', 'approach', 'problem', 'viewpoint', 'approach', 'assumption', 'datum', 'assume', 'joint_distribution', 'approach', 'model', 'datum', 'sample', 'probability', 'difference', 'choice', 'model', 'problem', 'mixture_model', 'asymmetric', 'clustering', 'model', 'mapping', 'concept', 'problem', 'mapping', 'show', 'searching', 'solution', 'problem', 'search', 'solution', 'space', 'input', 'distribution', 'sample', 'size', 'problem', 'case', 'problem', 'solution', 'review', 'ib', 'input', 'joint_distribution', 'distribution', 'representation', 'mapping', 'goal', 'value', 'joint_distribution', 'choice', 'probability', 'hf', 'hf_hi', 'principle', 'choice', 'choice', 'minimize', 'normalization', 'partition', 'function', 'kullback', 'leibler', 'divergence', 'equation', 'step', 'eq', 'define', 'algorithm', 'converge', 'review', 'ml', 'mixture', 'model', 'assume', 'value', 'distribution', 'denote', 'clustering', 'model', 'observation', 'distribution', 'model', 'process', 'label', 'increase', 'let', 'vector', 'define', 'label', 'topic', 'likelihood', 'aa', 'count', 'matrix', 'likelihood', 'choice', 'goal', 'estimation', 'assignment', 'parameter', 'likelihood', 'show', 'ml', 'count', 'algorithm', 'purpose', 'value', 'distribution', 'denote', 'step', 'distribution', 'reestimate', 'derivation', 'context', 'step', 'normalization', 'factor', 'step', 'converge', 'likelihood', 'likelihood', 'define', 'derivation', 'energy', 'functional', 'feg', 'step', 'respect', 'step', 'minimize', 'respect', 'condition', 'correspond', 'likelihood', 'fixed_point', 'ib', 'mapping', 'ml', 'problem', 'motivation', 'setting', 'purpose', 'mapping', 'problem', 'mapping', 'goal', 'mapping', 'show', 'equivalence', 'problem', 'show', 'algorithm', 'problem', 'case', 'mapping', 'distribution', 'mapping', 'assume', 'mapping', 'hf_hi', 'mapping', 'relation', 'eq', 'general', 'notice', 'distribution', 'ib', 'step', 'eq', 'step', 'decrease', 'phenomenon', 'map', 'consistency', 'problem', 'mapping', 'step', 'step', 'definition', 'likelihood', 'goal', 'issue', 'component', 'ml', 'trade', 'parameter', 'show', 'section', 'choice', 'purpose', 'sample', 'size', 'mapping', 'mapping', 'ib', 'step', 'step', 'mapping', 'direction', 'notice', 'mapping', 'search', 'solution', 'space', 'search', 'solution', 'space', 'ml', 'problem', 'observation', 'mapping', 'mapping', 'distribution', 'observation', 'result', 'fact', 'ib', 'step', 'eq', 'step', 'observation', 'mapping', 'ib', 'iterative', 'optimization', 'result', 'equivalence', 'step', 'step', 'uniform', 'notice', 'case', 'eq', 'eq', 'emphasize', 'equivalence', 'choice', 'ib', 'problem', 'value', 'freedom', 'ml', 'factor', 'claim', 'fixed_point', 'likelihood', 'fixed_point', 'corollary', 'uniformly_distributed', 'algorithm', 'induce', 'fixed_point', 'solution', 'maximize', 'minimize', 'proof', 'direction', 'ml', 'ib', 'direction', 'assume', 'observation', 'define', 'likelihood', 'result', 'step', 'observation', 'fixed_point', 'show', 'relationship', 'hf_hi', 'mapping', 'observation', 'hf_hi', 'multiplying', 'side', 'relation', 'hf_hi', 'mn', 'side', 'equivalence', 'value', 'corollary', 'iff', 'algorithm', 'result', 'proof', 'equivalence', 'energy', 'model', 'transformation', 'claim', 'special_case', 'uniform', 'claim', 'result', 'case', 'large_enough', 'je', 'claim', 'fixed_point', 'fixed_point', 'fixed_point', 'corollary', 'induce', 'fixed_point', 'solution', 'maximize', 'solution', 'result', 'clustering', 'mind', 'clustering', 'application', 'process', 'bias', 'application', 'ib', 'method', 'detail', 'iib', 'figure', 'value', 'proof', 'direction', 'ml', 'ib', 'direction', 'define', 'point', 'step', 'eq', 'extract', 'notice', 'mapping', 'ib', 'step', 'mapping', 'value', 'ib', 'iterative', 'show', 'mapping', 'algebra', 'algorithm', 'question', 'simulation', 'notice', 'value', 'claim', 'amount', 'uniformity', 'step', 'proof', 'situation', 'value', 'simulation', 'simulation', 'ib', 'lack', 'space', 'example', 'example', 'dkl', 'figure', 'general', 'mixture', 'model', 'ib', 'solution', 'space', 'sequence', 'probability', 'optimization', 'ml', 'space', 'sequence', 'probability', 'ib', 'space', 'result', 'paper', 'condition', 'sequence', 'subset', 'newsgroup', 'corpus', 'document', 'discussion', 'group', 'document', 'word', 'processing', 'goal', 'difference', 'value', 'datum', 'word', 'occurrence', 'document', 'dataset', 'document', 'cluster', 'process', 'mapping', 'iib', 'iteration', 'step', 'initialization', 'dataset', 'procedure', 'run', 'functional', 'functional', 'process', 'sample', 'size', 'difference', 'figure', 'value', 'iteration', 'convergence', 'run', 'value', 'value', 'ml', 'solution', 'ib', 'solution', 'phenomenon', 'sample', 'size', 'case', 'discussion', 'approach', 'condition', 'mind', 'assumption', 'joint_distribution', 'mixture_model', 'number', 'distribution', 'reason', 'probability', 'view', 'estimation', 'hand', 'solution', 'space', 'family', 'distribution', 'relation', 'distribution', 'input', 'formulation', 'minimization', 'formulation', 'family', 'distribution', 'mixture_model', 'assumption', 'sense', 'kl', 'respect', 'world', 'hand', 'ml', 'problem', 'world', 'minimize', 'kl', 'respect', 'distribution', 'analysis', 'procedure', 'case', 'map', 'ml', 'ib', 'concept', 'approach', 'example', 'ib', 'framework', 'large_enough', 'quality', 'solution', 'measure', 'purpose', 'model_selection', 'mapping', 'measure', 'estimation', 'problem', 'factor', 'general', 'component', 'ib', 'framework', 'form', 'solution', 'leave', 'issue', 'research', 'mixture_model', 'ml', 'ib', 'case', 'principle', 'model', 'equivalence', 'ib', 'method', 'multivariate', 'case', 'family', 'ib', 'problem', 'question', 'model', 'multivariate', 'ib', 'problem', 'direction', 'discussion', 'nir', 'friedman', 'reference', 'tishby', 'pereira', 'bialek', 'method', 'conference', 'communication', 'computation', 'slonim', 'information_bottleneck', 'theory', 'application', 'thesis', 'university', 'cover', 'nip', 'clustering', 'segmentation', 'pattern', 'recognition', 'letter', 'dempster', 'likelihood', 'datum', 'society', 'vol', 'clustering', 'image', 'segmentation', 'conference', 'computer', 'vision', 'eccv', 'int', 'friedman', 'tishby', 'document', 'classification', 'information', 'maximization', 'tishby', 'information_bottleneck', 'kl', 'respect', 'member', 'process', 'distribution', 'minimization', 'variable']]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Remove Stop Wordsa\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser'])\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN'])\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3108bfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 7), (4, 1), (5, 2), (6, 3), (7, 9), (8, 1), (9, 4), (10, 4), (11, 1), (12, 1), (13, 1), (14, 1), (15, 10), (16, 7), (17, 5), (18, 1), (19, 2), (20, 10), (21, 1), (22, 2), (23, 1), (24, 1), (25, 2), (26, 2), (27, 3), (28, 2), (29, 1), (30, 1), (31, 2), (32, 1), (33, 3), (34, 1), (35, 1), (36, 2), (37, 1), (38, 2), (39, 4), (40, 1), (41, 7), (42, 1), (43, 1), (44, 2), (45, 2), (46, 1), (47, 3), (48, 6), (49, 4), (50, 16), (51, 1), (52, 1), (53, 5), (54, 1), (55, 1), (56, 2), (57, 1), (58, 7), (59, 1), (60, 6), (61, 1), (62, 3), (63, 3), (64, 1), (65, 2), (66, 3), (67, 3), (68, 1), (69, 3), (70, 10), (71, 1), (72, 3), (73, 1), (74, 2), (75, 1), (76, 2), (77, 2), (78, 3), (79, 3), (80, 5), (81, 1), (82, 2), (83, 1), (84, 5), (85, 24), (86, 1), (87, 2), (88, 1), (89, 1), (90, 2), (91, 4), (92, 4), (93, 1), (94, 4), (95, 1), (96, 1), (97, 2), (98, 2), (99, 2), (100, 1), (101, 7), (102, 3), (103, 1), (104, 2), (105, 1), (106, 1), (107, 2), (108, 1), (109, 1), (110, 1), (111, 1), (112, 14), (113, 1), (114, 2), (115, 28), (116, 1), (117, 1), (118, 2), (119, 1), (120, 2), (121, 1), (122, 1), (123, 5), (124, 2), (125, 2), (126, 4), (127, 4), (128, 5), (129, 14), (130, 1), (131, 11), (132, 1), (133, 1), (134, 1), (135, 1), (136, 2), (137, 2), (138, 1), (139, 1), (140, 1), (141, 1), (142, 2), (143, 5), (144, 1), (145, 1), (146, 7), (147, 1), (148, 2), (149, 1), (150, 2), (151, 4), (152, 3), (153, 1), (154, 1), (155, 2), (156, 1), (157, 1), (158, 2), (159, 7), (160, 25), (161, 2), (162, 5), (163, 1), (164, 4), (165, 4), (166, 2), (167, 1), (168, 2), (169, 1), (170, 1), (171, 1), (172, 1), (173, 3), (174, 1), (175, 5), (176, 1), (177, 5), (178, 8), (179, 2), (180, 2), (181, 6), (182, 1), (183, 1), (184, 1), (185, 3), (186, 1), (187, 1), (188, 2), (189, 1), (190, 3), (191, 1), (192, 1), (193, 7), (194, 2), (195, 3), (196, 1), (197, 5), (198, 1), (199, 1), (200, 16), (201, 2), (202, 8), (203, 1), (204, 21), (205, 1), (206, 1), (207, 1), (208, 1), (209, 4), (210, 1), (211, 3), (212, 2), (213, 2), (214, 1), (215, 1), (216, 1), (217, 16), (218, 2), (219, 1), (220, 1), (221, 1), (222, 1), (223, 1), (224, 1), (225, 1), (226, 2), (227, 2), (228, 1)]]\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54bb8738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=10, \n",
    "                                       random_state=100,\n",
    "                                       chunksize=100,\n",
    "                                       passes=10,\n",
    "                                       per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18a78bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.019*\"eigenvector\" + 0.019*\"probability\" + 0.018*\"edge\" + 0.016*\"cluster\" '\n",
      "  '+ 0.015*\"matrix\" + 0.013*\"function\" + 0.012*\"cost_function\" + '\n",
      "  '0.012*\"result\" + 0.012*\"sensitivity\" + 0.011*\"number\"'),\n",
      " (1,\n",
      "  '0.017*\"ctw\" + 0.017*\"stimulus\" + 0.012*\"model\" + 0.010*\"datum\" + '\n",
      "  '0.010*\"estimate\" + 0.009*\"result\" + 0.009*\"time\" + 0.009*\"memory\" + '\n",
      "  '0.009*\"assumption\" + 0.008*\"number\"'),\n",
      " (2,\n",
      "  '0.001*\"ctw\" + 0.001*\"probability\" + 0.001*\"datum\" + 0.001*\"model\" + '\n",
      "  '0.001*\"matrix\" + 0.001*\"number\" + 0.001*\"eigenvector\" + 0.001*\"cluster\" + '\n",
      "  '0.001*\"edge\" + 0.001*\"problem\"'),\n",
      " (3,\n",
      "  '0.001*\"channel\" + 0.001*\"model\" + 0.001*\"datum\" + 0.001*\"probability\" + '\n",
      "  '0.001*\"problem\" + 0.001*\"cell\" + 0.001*\"assumption\" + 0.001*\"cluster\" + '\n",
      "  '0.001*\"function\" + 0.001*\"number\"'),\n",
      " (4,\n",
      "  '0.021*\"channel\" + 0.020*\"model\" + 0.018*\"brain\" + 0.017*\"datum\" + '\n",
      "  '0.014*\"community\" + 0.014*\"graph\" + 0.013*\"network\" + 0.011*\"estimation\" + '\n",
      "  '0.011*\"structure\" + 0.009*\"problem\"'),\n",
      " (5,\n",
      "  '0.001*\"model\" + 0.001*\"channel\" + 0.001*\"problem\" + 0.001*\"probability\" + '\n",
      "  '0.001*\"datum\" + 0.001*\"tensor\" + 0.001*\"case\" + 0.001*\"function\" + '\n",
      "  '0.001*\"solution\" + 0.001*\"step\"'),\n",
      " (6,\n",
      "  '0.048*\"tensor\" + 0.021*\"cluster\" + 0.018*\"gram\" + 0.017*\"datum\" + '\n",
      "  '0.017*\"group\" + 0.017*\"method\" + 0.015*\"probability\" + 0.014*\"state\" + '\n",
      "  '0.013*\"graph\" + 0.013*\"index\"'),\n",
      " (7,\n",
      "  '0.001*\"model\" + 0.001*\"channel\" + 0.001*\"datum\" + 0.001*\"cell\" + '\n",
      "  '0.001*\"result\" + 0.001*\"number\" + 0.001*\"problem\" + 0.001*\"stimulus\" + '\n",
      "  '0.001*\"probability\" + 0.001*\"solution\"'),\n",
      " (8,\n",
      "  '0.032*\"mapping\" + 0.029*\"problem\" + 0.028*\"ib\" + 0.024*\"step\" + '\n",
      "  '0.019*\"value\" + 0.019*\"solution\" + 0.019*\"distribution\" + '\n",
      "  '0.016*\"likelihood\" + 0.016*\"ml\" + 0.013*\"model\"'),\n",
      " (9,\n",
      "  '0.060*\"cell\" + 0.049*\"channel\" + 0.047*\"call\" + 0.017*\"policy\" + '\n",
      "  '0.016*\"time\" + 0.016*\"system\" + 0.015*\"rl\" + 0.014*\"problem\" + '\n",
      "  '0.012*\"configuration\" + 0.012*\"allocation\"')]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1dc5dc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.39210503030051375\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import CoherenceModel\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b8c150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
